\section{Overview}

\paragraph{Motivation}

Swan sucks: too much variance on its performance depending on dataset quality/quantity

\subsection{Overview of our System}

% 적당한 figure 하나 넣자

Figure 1 shows the overall workflow of SpecHunter. Given a Java application's
source code, SpecHunter labels all API methods provided by the library the application
uses. There are three main issues regarding building an effective interactive system:
namely, constructing the network, effectively propagating the evidence given from the
oracle, and making the system scale to run on large input applications.
% Question: Should we define what "API" means?

\subsection{Network Construction}

\paragraph{Determining Edges}

% Network Construction은 반드시 Scalability and Efficiency와 연관되게 되어 있다.
% 그 점을 유념하면서 쓰자: 적절한 pointer 달아 주기.
Here we describe how we represent the input java project into a large graph form, which will
later be turned into a series of Bayesian networks. How we splitt this large
graph into several Bayesian networks will be discussed in the next section. % TODO 좀 더 자연스러운 포인터??
We hope to create a graph $G=(V, E)$,
where $V$ is the set of all methods used or defined in the given application, and $E$ is a
subset of $V\times V$, whose elements are gathered in three different ways. 

First, there are data-flow-edges. We perform a variant of a data-flow analysis on the
given input application to calculate lifetimes of all access paths found in the
entire codes, starting from being defined and ending by being ``dead'', between
which the access path may be ``redefined''.
% 여기서 간단한 예제 넣어주기

\lstinputlisting[language=Java]{./Codes/SimpleExample.java}

In the above example, a variable \texttt{x} is being defined to hold the value \texttt{1} in the method
\texttt{f}, using another method \texttt{m1}. This value flows to \texttt{g}, where this
value is redefined as \texttt{2} using the method \texttt{m2}. After this redefined
value flows to method \texttt{m3} via \texttt{h}, it is no longer used after the call to
\texttt{println}. Here, our goal is to capture this lifecycle. To compute these
for all access paths in the program, we first run a static analysis designed as follows:

\begin{align*}
  A &\in \mathbb{D} &= \mathbb{C}\rightarrow\mathcal{P}(State)\\
  s &\in State &= Procname\times Var\times Loc\times Alias\\
  p &\in Procname &\subseteq Var\\
  a &\in Alias &\subseteq \mathcal{P}(Var)
\end{align*}

For example, for an assignment statement \texttt{int a = b;} inside a procedure
\texttt{f} at line 3, we obtain the tuple set \texttt{\{(f, a, 3, \{a, b\})\}}.
The fourth component of the tuple means that \texttt{a} and \texttt{b} are
aliases. After we create such tuples for each program point, we finally construct
\emph{propagation chains} by threading the tuples via alias relations. The
relevant generated tuples for our goal is:

\begin{itemize}
\item \texttt{\{(f, x, 14, \{x, y\})\}}
\item \texttt{\{(g, y, 18, \{y, u1\})\}}
\item \texttt{\{(m2, u1, 5, \{u1\})\}}
\item \texttt{\{(m2, u1, 6, \{u1\})\}}
\item \texttt{\{(g, z, 6, \{z, u1, w\})\}}
\item \texttt{\{(g, w, 23, \{w, u2\})\}}
\item \texttt{\{(m3, u2, 9, \{u2, println\_1\})\}}
\end{itemize}

>
where \texttt{println\_1} is a Mangled parameter variable of \texttt{println}.
Now, connecting the tuples with the aliases, we get two chains:
\texttt{x -> y -> u1} and \texttt{u1 -> z -> w -> u2 -> println\_1}. Since \texttt{u1} is
redefined in \texttt{m2}, we glue the two together to get the originally desired
information: \texttt{x -> y -> u1 -> z -> w -> u2 -> println\_1}. This translates
into data-flow edges: \texttt{(f, g)}, \texttt{(f, h)}, and \texttt{(m3, println)}.

% TODO 아 이거 실제로 돌려봐서 확인해야 할 거 같은데.. 실제로 이렇게 나오나...?


% TODO static callee가 무엇인지를 좀 알아와서 디테일을 추가할 것.
Second, there are call-edges. Since we used Facebook's Infer (fbinfer.com/) to
implement our static analysis, we used Infer's facility to compute static
callees of a given method to draw the entire callgraph of a given application
code. For each caller \texttt{f} and its callee \texttt{g}, we add an edge
\texttt{(f, g)}.


Last but not least, there are similarity-edges. The idea is to take any two
methods and score its pairwise similarity, and leave only the pairs with score
above a predefined threshold. We measure them against two large groups of
criteria: the ``syntactic features'' and ``semantic features''. For the
syntactic features, we borrrowed largely from \textbf{Swan}'s features that
check how names of methods or their return types are composed. The below
figure % TODO: Figure 번호 달기...
shows all the features we used.

% TODO 표 삐져나간다..!!!!
\begin{center}
  \begin{tabular} { |c|c|c| }
    \hline
    Syntactic & Semantic\\
    \hline
    \hline
    Both has parameters? & Both has at least one same callee?\\
    Both has return type? & Both are making calls but not passing data?\\
    Both contains same word in method name? & Both are being called but not being passed data?\\
    Both contains same word in class name? & Both are making data flow calls?\\
    Both has return type contained in parameter type? & Both are being passed data and passing data simultaneously?\\
    Both has same return type? & Both are being passed data which gets dead in them?\\
    Both has return type contained in method name? & Both has variable redefinitions?\\
    \hline
  \end{tabular}
\end{center} 

By combining all three kinds of edges, we get a finished graph where every node
is represents a node, and every edge represent how one method relates to another.

\paragraph{Determining CPTs}

After we determine which and which methods should be connected with a directed
edge, we then define a conditional probability table (CPT) for each directed
edge. This step is essential since CPTs are essential components of Bayesian
Networks and we want our big graph to be broken down into small
Bayesian networks. Each node should have its own CPT, and it express how the
node's marginal distribution depends
on those of the parent nodes. We use largely two different CPT schemas:
that of the dataflow-edges and those of the call- and similarity-edges.
Regardless of the schemas, the CPT's size is $4\times 4^n$, where $n$ is the
number of parent nodes of a node. The below figure demonstrates a CPT of a node
with two parent nodes, each shooting a data-flow edge to this node.

% TODO 텍스트로만 써 놓은 걸 실제로 색칠하기.
% VH -> Red, High -> Orange,
% Low -> Green, VL -> Blue
\begin{center}
  \begin{tabular} { |c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c| }
    \hline
    vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl  \\
    high & vl   & high & high & vl   & vl   & vl   & vl   & high & vl   & high & high & high & vl   & high & high\\
    high & vl   & vl   & high & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & high & vl   & vl   & high\\
    high & vl   & high & high & vl   & vl   & vl   & vl   & high & vl   & high & high & high & vl   & high & high\\
    \hline
  \end{tabular}
\end{center}

Let us first state with the data-flow schemas. In the above table, red cells
should contain very high probabilities, orange
cells should contain relatively high probabilities, whereas green cells should
be assigned low probabilities, and blue cells should be of very low probabilities.
The idea is this: by the definitions of ``source'', ``sanitizer'', and ``sink'',
data must flow from a source to a sink through possibly a sanitizer, but it
cannot flow in any other sequence of such methods. (For example, data cannot
possibly flow from a sink through a source to a sanitizer.) Also, we note that
any number of ``none'' methods, which are none of the three, can be interleaved
freely between a source and a sanitizer, and between a sanitizer and a sink.
Therefore, we encode these possibilities into a tabular form containing probabilities
like the above.

For the call-edges and similarity-edges, we use the same schema for both. The
below figure shows how we assign probabilities when a node has two parents, each
shooting call-edges or similarity-edges to the node.

% TODO 텍스트로만 써 놓은 걸 실제로 색칠하기.
% VH -> Red, High -> Orange,
% Low -> Green, VL -> Blue
\begin{center}
  \begin{tabular} { |c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c| }
    \hline
    high & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl  & vl   & vl   & vl  \\
    vl   & vl   & vl   & vl   & vl   & high & vl   & vl   & vl   & vl   & vl   & vl   & vl  & vl   & vl   & vl  \\
    vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & high & vl   & vl  & vl   & vl   & vl  \\
    vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl   & vl  & vl   & vl   & high\\
    \hline
  \end{tabular}
\end{center}
% TODO 구현 상으로는 위의 high cell이 low cell로 되어 있는데, 나중에 *꼭*
% 바꾸자. high cell로 두는 게 맞는 것 같다.

% explanation: why do we make similarity edges share this schema with call edges?
Combining these together, we can express the case of a node with two parents,
one shooting a data-flow edge while another shoots a call- or similarity edge.

% TODO 텍스트로만 써 놓은 걸 실제로 색칠하기.
% VH -> Red, High -> Orange,
% Low -> Green, VL -> Blue
\begin{center}
  \begin{tabular} { |c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c| }
    \hline
    low  & vl   & vl   & vl   & low  & vl   & vl   & vl   & low  & vl   & vl   & vl   & low  & vl   & vl   & vl  \\
    high & low  & high & high & vl   & low  & vl   & vl   & high & low  & cell & high & high & low  & high & high\\
    high & vl   & low  & high & vl   & vl   & low  & vl   & vl   & vl   & low  & vl   & high & vl   & low  & high\\
    high & vl   & high & vh   & vl   & vl   & vl   & low  & high & vl   & high & vh   & high & vl   & high & vh  \\
    \hline
  \end{tabular}
\end{center}
% TODO 여기도 구현 상으로는 위의 high cell이 low cell로 되어 있는데, 나중에 *꼭*
% 바꾸자. high cell로 두는 게 맞는 것 같다.

Since a CPT has a constraint that the sum of probabilities assigned to each cell
of a column is 1, we can come up with the following formula to calculate a
concrete probability value for a cell's relative magnitude of probability.
Suppose we call four cells of a column $cell_1$, $cell_2$, $cell_3$, $cell_4$,
then the probability value of one of the four cells is:

\begin{equation*}
  prob(cell) = \frac{mag(cell)\times coeff(mag(cell))}{\sum_{i=1}^{4}mag(cell)\times coeff(mag(cell_i))}
\end{equation*}

where $mag$ is the score based on relative magnitude: we give $1$ to cells whose
probability should be ``very low'', and $4$ to cells whose probability should be
``very high''. $coeff$ of a relative magnitude is a constant that is multiplied
to the magnitude score. It helps to widen the gap between probabilities assigned
to cells in a column, bearing different relative magnitudes.

\subsection{Graph Manipulation}

% TODO cite networkx 
Here, we explain how we cut the big graph into a handful of small ones, and
further preprocess them before turning each into a Bayesian network. We used
\emph{networkx}'s implementation for each core algorithms: Kernighan-Lin
bisection and cycle detection.

\paragraph{Splitting Graphs}

% TODO: self_question_n_answer.py가 interaction 한번에 얼마나 걸리는지를 측정해
% 그래프로 그리기
After we are done making the initial graph, we chop it down to small graphs of
size under a certain threshold, which will later be converted into Bayesian
networks. This breaking down is necessary since the time consumed during loopy
belief propagation and D-separation algorithms, which will be discussed later
in the next section, gets increasingly inefficient as the graph size increases. Since we are aiming
to reduce the time for a single interaction under \emph{xx??} seconds, we
decided that the optimal threshold for a graph is \emph{180??}. The algorithm
for this task is described in figure \emph{??}. This algorithm relies heavily on
the Kernighan-Lin partitioning algorithm, % TODO: should we cite the algorithm's paper?
which partitions the given graph $G=(V, E)$ into two graphs $G_1=(V_1,E_1)$ and
$G_2=(V_2, E_2)$, such that $V_1$ and $V_2$ are almost equal and the edges
connecting $G_1$ and $G_2$ are minimized. % TODO: at least skim the Kernighan-Lin paper!
% is it really 'minimized'??

\paragraph{Decycling, and Controlling in-edges}

Since Bayesian networks are directed acyclic graphs (DAGs), we need to eliminate
all cycles before we turn the splitted graphs into Bayesian networks. We first
use \emph{networkx}'s cycle
detection algorithm which pinpoints the set of edges forming a cycle by
searching the graph in a depth-manner for this task. We then randomly pick one of
such edges causing a cycle and delete it. Repeatedly applying the two
operations until the algorithm fails to detect a cycle, we transform the
splitted graphs into DAGs.      % TODO 이거 figure로 굳이 뺄 필요 없겠지...?

% 'incoming edge가 너무 많아서도, 너무 적어서도 안 된다.'
Next, we control the number of incoming edges of each node. It is already
mentioned that the size of a node's CPT is exponential to the number of its
parent nodes. To our empirical knowledge, if the number of incoming edges (hence
the number of parent nodes) becomes more than 6, the process of converting the
graph into a Bayesian network becomes drastically intractable. Therefore, we
use a simple algorithm to deal with those. 

After all these processing, we are finally ready to turn chopped graphs into
small, yet full-blown Bayesian networks.

\subsection{Information Propagation}

Here, we expound on the core inference algorithm. 