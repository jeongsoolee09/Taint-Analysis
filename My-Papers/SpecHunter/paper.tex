%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[sigconf,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
% \documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
% \documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
% \documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
% \documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
% \setcopyright{acmcopyright}
% \setcopyright{acmlicensed}
% \setcopyright{rightsretained}
% \copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
%% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
%% http://ctan.org/pkg/subcaption

\usepackage{listings}

\lstset{
  language=Java,
  numbers=left,
  stepnumber=1,
  firstnumber=1,
  numberfirstline=true
}

\begin{document}

%% Title information
\title{SpecHunter: Interactively Inferring Taint Specifications Using Bayes Net}        
%% when present, will be used in
%% header instead of Full Title.
\titlenote{with title note}             %% \titlenote is optional;
%% can be repeated if necessary;
%% contents suppressed with 'anonymous'
\subtitle{Subtitle}                     %% \subtitle is optional
\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
%% can be repeated if necessary;
%% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
%% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
% \author{First2 Last2}
% \authornote{with author2 note}          %% \authornote is optional;
%                                         %% can be repeated if necessary
% \orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
% \affiliation{
% \position{Position2a}
% \department{Department2a}             %% \department is recommended
% \institution{Institution2a}           %% \institution is required
% \streetaddress{Street2a Address2a}
% \city{City2a}
% \state{State2a}
% \postcode{Post-Code2a}
% \country{Country2a}                   %% \country is recommended
% }
%   \email{first2.last2@inst2a.com}         %% \email is recommended
%   \affiliation{
%   \position{Position2b}
%   \department{Department2b}             %% \department is recommended
%   \institution{Institution2b}           %% \institution is required
%   \streetaddress{Street3b Address2b}
%   \city{City2b}
%   \state{State2b}
%   \postcode{Post-Code2b}
%   \country{Country2b}                   %% \country is recommended
% }
%   \email{first2.last2@inst2b.org}         %% \email is recommended


%%   Abstract
%%   Note: \begin{abstract}...\end{abstract} environment must come
%%   before \maketitle command
\begin{abstract}
  SpecHunter is a tool for inferring taint specifications, aiming to aid anyone
  trying to use a Java taint analyzer, but being overwhelmed by the number of APIs that
  should be marked of their specifications to run it. SpecHunter aims to lessen this burden
  by constructing a Bayesian network and performing live interaction with the user, in which
  it systematically picks a method and asks the user of its specification. By taking an
  interactive approach, SpecHunter can run on Java applications that use libraries that are
  implemented in languages other than Java. Also, SpecHunter skips all trial-and-errors
  in configurating the data-set required in using any of the traditional machine-learning approaches.
  Our experiments show that SpecHunter actually delivers those promises.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10011007.10011006.10011008</concept_id>
  <concept_desc>Software and its engineering~General programming languages</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10003456.10003457.10003521.10003525</concept_id>
  <concept_desc>Social and professional topics~History of programming languages</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{keyword1, keyword2, keyword3}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

\section{Introduction}
Taint analysis, whether static or dynamic, aims to find security vulnerabilities lurking
in the given program. These vulnerabilities include SQL injection, cross-site
scripting, and abused printf-arguments. They are typically formulated as a
data flow starting from a `source' method to a `sink' method, without the
flowing data being `sanitized' by a relevant method. For such search to be
precise enough, the analyzer must first be told which methods are sources,
sinks, sanitizers, or none of them. However, going through the codebase
searching for imported APIs and labelling every one of them by hand quickly becomes tedious
thing to do as the codebase gets larger.

% TODO BibTeX을 쓸 줄 모르므로 일단 툴 이름으로 Reference로의 링크를 대신함
% Boldface들을 BibTeX으로 바꿀 것.
To aid such manpower, previous studies proposed methods such as supervised
learning (\textbf{SuSI}, \textbf{SWAN}), and semi-supervised learning (\textbf{Seldon}, \textbf{Merlin}).
However, works belonging to each camp suffers from their own limitations.

\paragraph{Shortcomings of Supervised Learning Camp.}

\textbf{SuSI} and \textbf{SWAN} both work in a similar way: they first train a learning model
such as a support vector machine with the actual library codes implementing
APIs. Here, the function bodies are interprocedurally analyzed with a Java
static analyzer to get the vectors of feature values. Then, they take a whole
library codebase into the pre-trained learning
model, and get the mapping from all APIs in the library to their respective
specifications. While they work nicely against libraries written in pure Java
code such as Spring, they cannot work if a given application imports libraries
written in other languages other than Java. This is because in order to extract
the feature values they would have to be equipped with all the possible
languages the libraries would be in, and this is practically difficult.

% TODO: add an additional shortcoming: features should also be updated if a new
% library is provided

\paragraph{Shortcomings of Semi-Supervised Learning Camp.}

As a workaround to the above limitation, one can try a semi-supervised learning
approach. The state-of-the-art belonging to this camp is \textbf{Seldon}.
\textbf{Seldon} only takes the application code into consideration, thus it does
not care which language the library is written in. So, for example, Seldon can
figure out the label of an API function in a Python program, which is
actually implemented in C++. However, Seldon suffers from a common problem of
all machine-learning solutions, that it is largely dependent on quality dataset.
In fact, the authors reports that \textbf{Seldon}'s precision dropped by 14
percent-points by just providing half of the original specifications. Moreover,
this problem is shared with supervised learning solutions, those are also
data-driven solutions.
% Seldon 논문의 부분을 인용하고 싶은데... 형식을 모르겠네
% 해당 부분:
% Q6: Impact of Seed Specification. We also evaluated Sel-
% don’s precision for half the seed specification (considering
% only odd line numbers in App. B). This significantly reduces
% precision, by 14 percentage-points. Thus, we believe our
% seed specification strikes a good balance between manual
% effort and precision. We note that in the extreme case of an
% empty seed specification, Seldon will predict 0 specifications,
% because picking 0 for all variables is a trivial solution to its
% constraint system.

\paragraph{Our Tool: Real-Time Interactive Inference.}

Our tool, SpecHunter, works in a different manner than data-driven
approaches. Instead of training the model with some predetermined pairs of
methods and their labels to predict other methods' labels, we analyze the given
application to construct probabilistic graphical models of a certain type,
called Bayesian networks. They are special in that they provide a model of
conditional-probabilistic reasoning, which infers the marginal probabilities of
each random variables when some of them are instantiated to a certain value by
an external source. Therefore, we first determine a structure of a Bayesian network
by running a static analysis on the given application code, and make the system ask
the user, the external entity, for evidence. Then, SpecHunter picks the most
influential node which has the most dependent nodes. This makes the interaction
session both effective and efficient.

The experiments show that SpecHunter effectively infers taint specifications for
methods that are untold of its label, with \textbf{xx}\% precision when told only
\textbf{17.7}\% of all APIs.


\paragraph{Contributions.}

\begin{itemize}
  \item We provide SpecHunter
  \item We provide src/sin/san/non ground truths
  \item We provide a taint analyzer (is it possible?)
  \item We make the tool open source (is it possible?)
\end{itemize}

\section{Overview}

\paragraph{Motivation}

Swan sucks: too much variance on its performance depending on dataset quality/quantity

\subsection{Overview of our System}

% 적당한 figure 하나 넣자

Figure 1 shows the overall workflow of SpecHunter. Given a Java application's
source code, SpecHunter labels all API methods provided by the library the application
uses. There are three main issues regarding building an effective interactive system:
namely, constructing the network, effectively propagating the evidence given from the
oracle, and making the system scale to run on large input applications.
% Question: Should we define what "API" means?

\subsubsection{Network Construction}

\paragraph{Determining Edges}

% Network Construction은 반드시 Scalability and Efficiency와 연관되게 되어 있다.
% 그 점을 유념하면서 쓰자: 적절한 pointer 달아 주기.
Here we describe how we represent the input java project into a large graph form, which will
later be turned into a series of Bayesian networks. How we splitt this large
graph into several Bayesian networks will be discussed in the next section. % TODO 좀 더 자연스러운 포인터??
We hope to create a graph $G=(V, E)$,
where $V$ is the set of all methods used or defined in the given application, and $E$ is a
subset of $V\times V$, whose elements are gathered in three different ways. 

First, there are data-flow-edges. We perform a variant of a data-flow analysis on the
given input application to calculate lifetimes of all access paths found in the
entire codes, starting from being defined and ending by being ``dead'', between
which the access path may be ``redefined''.
% 여기서 간단한 예제 넣어주기

\lstinputlisting[language=Java]{./Codes/SimpleExample.java}

In the above example, a variable \texttt{x} is being defined to hold the value \texttt{1} in the method
\texttt{f}, using another method \texttt{m1}. This value flows to \texttt{g}, where this
value is redefined as \texttt{2} using the method \texttt{m2}. After this redefined
value flows to method \texttt{m3} via \texttt{h}, it is no longer used after the call to
\texttt{println}. Here, our goal is to capture this lifecycle. To compute these
for all access paths in the program, we first run a static analysis designed as follows:

% 무슨 environment를 써야 하나...? 일단 적어보쟈

\begin{align*}
  A\in \mathbb{D} = \mathbb{C}\rightarrow\mathcal{P}(State)\\
  s\in State = Procname\times Var\times Loc\times Alias\\
  p\in Procname \subseteq Var\\
  a\in Alias \subseteq \mathcal{P}(Var)
\end{align*}

For example, for an assignment statement \texttt{int a = b;} inside a procedure
\texttt{f} at line 3, we obtain the tuple set \texttt{\{(f, a, 3, \{a, b\})\}}.
The fourth component of the tuple means that \texttt{a} and \texttt{b} are
aliases. After we create such tuples for each program point, we finally construct
\emph{propagation chains} by threading the tuples via alias relations. The
relevant generated tuples for our goal is:

\begin{itemize}

\item \texttt{\{(f, x, 14, \{x, y\})\}}
\item \texttt{\{(g, y, 18, \{y, u1\})\}}
\item \texttt{\{(m2, u1, 5, \{u1\})\}}
\item \texttt{\{(m2, u1, 6, \{u1\})\}}
\item \texttt{\{(g, z, 6, \{z, u1, w\})\}}
\item \texttt{\{(g, w, 23, \{w, u2\})\}}
\item \texttt{\{(m3, u2, 9, \{u2, println_1\})\}}
    
\end{itemize}

where \texttt{println_1} is a Mangled parameter variable of \texttt{println}.
Now, connecting the tuples with the aliases, we get two chains: \texttt{x -> y
  -> u1} and \texttt{u1 -> z -> w -> u2 -> println_1}. Since \texttt{u1} is
redefined in \texttt{m2}, we glue the two together to get the originally desired
information: \texttt{x -> y -> u1 -> z -> w -> u2 -> println_1}. This translates
into data-flow edges: \texttt{(f, g)}, \texttt{(f, h)}, and \texttt{(m3, println)}.

% 아 이거 실제로 돌려봐서 확인해야 할 거 같은뎅.. 실제로 이렇게 나오나...?


% TODO static callee가 무엇인지를 좀 알아와서 디테일을 추가할 것.
Second, there are call-edges. Since we used Facebook's Infer (fbinfer.com/) to
implement our static analysis, we used Infer's facility to compute static
callees of a given method to draw the entire callgraph of a given application
code. For each caller \texttt{f} and its callee \texttt{g}, we add an edge
\texttt{(f, g)}.


Last but not least, there are similarity-edges. The idea is to take any two
methods and score its pairwise similarity, and leave only the pairs with score
above a predefined threshold.
% TODO syntactic, semantic feature 표 넣기

\paragraph{Determining CPTs}

% 여기서는 표를 좀 빡시게 그려야 한다. 셀도 좀 색칠하고...


\subsubsection{Graph Splitting}



\subsubsection{Information Propagation}


\section{Evaluation}

\subsection{RQs}
\begin{itemize}
\item How effective is SH in finding src/sin/san?  % I am really really worried about this
\item How efficient is SH?
\item Interaction user-study % possible?
\end{itemize}

% why is this meaningful? (with/without comparison)
% at least mention comparison

\paragraph{Discussions}
How well are propagations working? (Quant, Qualit) .
How stable is SpecHunter over multiple iterations?
\paragraph{Limitations}

\paragraph{Threats to Validity}


\section{Related Work}


\section{Conclusion}


%% Acknowledgments
\begin{acks}                            %% acks environment is optional
  %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}. Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}


%% Bibliography
% \bibliography{bibfile}


%% Appendix
\appendix
\section{Appendix}

Text of appendix \ldots

\end{document}
